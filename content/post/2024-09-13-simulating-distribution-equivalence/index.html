---
title: Simulating Distribution Equivalence
author: Conor Neilson
date: '2024-09-13'
slug: simulating-distribution-equivalence
categories:
  - R
tags:
  - R
  - simulation
featureImage: images/nelson_lakes_photo.jpg
---



<p>Been a long time since I did any simulation in R! Today I was presented with a nice opportunity too.</p>
<p>I received a message from my brother today saying “<code>sqrt(rand())</code> gives you the same probabilities as <code>max(rand(), rand())</code> assuming <code>rand()</code> returns a number between 0 and 1.” I was immediately intrigued as this didn’t make intuitive sense to me. However, no need for intuition! This is a data-driven blog, so lets look at this in a data-driven way.</p>
<p>My goal here is to create a couple of simulations that simulate <code>sqrt(rand())</code> and <code>max(rand(), rand())</code> and see what their distributions look like.</p>
<p>I’ll start by loading a couple of useful packages and set a seed for reproducibility.</p>
<pre class="r"><code>library(dplyr)
library(ggplot2)
library(tidyr)
set.seed(42)</code></pre>
<p>I like to follow <a href="http://varianceexplained.org/">David Robinson’s examples</a> here and treat simulations as mutating a dataframe. To do this, I start by setting up three columns of randomly generated numbers in a dataframe, then use <code>dplyr</code> to create new columns with my transformed <code>sqrt</code> and <code>max</code> values.</p>
<p>Note that R does not actually have a <code>rand()</code> function. Instead, for generating a uniform distribution of points between 0 and 1 we can use the <code>runif()</code> function with its default parameters.</p>
<p>Also, we use <code>pmax</code> rather than <code>max</code> so we can compare multiple vectors.</p>
<pre class="r"><code>N &lt;- 100000


df &lt;- tibble(
  num_1 = runif(N),         
  num_2 = runif(N),        
  num_3 = runif(N)         
) %&gt;%
  mutate(
    sqrt_rand = sqrt(num_1),                
    max_rand = pmax(num_2, num_3)             
  )


df_long &lt;- df %&gt;%
  select(sqrt_rand, max_rand) %&gt;%
  pivot_longer(cols = everything(), names_to = &quot;variable&quot;, values_to = &quot;value&quot;)

head(df_long)</code></pre>
<pre><code>## # A tibble: 6 × 2
##   variable  value
##   &lt;chr&gt;     &lt;dbl&gt;
## 1 sqrt_rand 0.956
## 2 max_rand  0.701
## 3 sqrt_rand 0.968
## 4 max_rand  0.341
## 5 sqrt_rand 0.535
## 6 max_rand  0.497</code></pre>
<p>Excellent, looks like we’ve got the data we need. All that’s needed now is to plot it a few different ways. I’m going to create a histogram, a <a href="https://en.wikipedia.org/wiki/Empirical_distribution_function">ecdf</a>, and a <a href="https://en.wikipedia.org/wiki/Probability_density_function">pdf</a></p>
<pre class="r"><code>ggplot(df_long, aes(x = value, fill = variable)) +
  geom_histogram(alpha = 0.5, position = &#39;identity&#39;, bins = 50) +
  labs(title = &quot;Histograms of sqrt(runif()) and max(runif(), runif())&quot;, x = &quot;Value&quot;, y = &quot;Count&quot;) </code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>ggplot(df_long, aes(x = value, color = variable)) +
  stat_ecdf() +
  labs(title = &quot;ECDFs of sqrt(runif()) and max(runif(), runif())&quot;, x = &quot;Value&quot;, y = &quot;ECDF&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>ggplot(df_long, aes(x = value, color = variable)) +
  geom_density() +
  labs(title = &quot;PDFs of sqrt(runif()) and max(runif(), runif())&quot;, x = &quot;Value&quot;, y = &quot;Density&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-5-1.png" width="672" />
There you have it - it seems pretty clear from these plots that these are following almost identical distributions. The ECDF in particular is almost impossible to see there are two lines there.</p>
<p>We can run a Kolmogorov-Smirnov test to check whether these are the same distribution, however, the KS test is extremely sensitive to sample size, and with 100,000 data points we are practically guaranteed to get a high p-value. We’ll run it anyway just for fun.</p>
<pre class="r"><code>ks_result &lt;- ks.test(df$sqrt_rand, df$max_rand)
ks_result</code></pre>
<pre><code>## 
## 	Asymptotic two-sample Kolmogorov-Smirnov test
## 
## data:  df$sqrt_rand and df$max_rand
## D = 0.00414, p-value = 0.3582
## alternative hypothesis: two-sided</code></pre>
<p>Yeah, high p-value, providing evidence for the null hypothesis (there is no significant difference between the two distributions).</p>
<p>I’d really like to dig into this and work out the maths behind why these are the same distribution and also explore for different parameter values for <code>runif()</code>. Unfortunately, this is all I have time for tonight. Maybe there will be follow up post…</p>
